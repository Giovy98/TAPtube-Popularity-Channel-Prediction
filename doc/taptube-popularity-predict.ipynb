{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TapTube Popularity Channel Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<center> <img src=\"images/potarataptube.jpg\"  /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo tutorial, costruiremo un sistema di predizione in tempo reale per valutare la popolarità di un canale YouTube basandoci su vari parametri, che verranno descritti nella sezione ***Pipeline*** in particolare nella sottosezione ***Producer*** . Questo ci permetterà di distinguere tra canali attivi e inattivi, nonché tra creatori di contenuti e non."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/struttura.png\"  /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Le tecnologie utilizzate in questo progetto sono:\n",
    "1. Uno script in Python per raccogliere i dati relativi a YouTube: Producer\n",
    "2. Uno strumento per l'ingestion dei dati: Logstash.\n",
    "3. Uno strumento per lo streaming dei dati: Apache Kafka.\n",
    "4. Uno strumento per il processamento dei dati: Spark Structured Streaming (Apache Spark) con il supporto di **MLlib** per l'analisi e il machine learnine e **Databricks** per testing\n",
    "5. Uno strumento per l'indicizzazione dei dati: Elasticsearch.\n",
    "6. Uno strumento per la visualizzazione dei dati: Kibana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel mio progetto, ho utilizzato un script denominato tubescraper.py per interagire con l'API pubblica di YouTube (allego il link https://developers.google.com/youtube/v3?hl=it) e ottenere i dati necessari. Questa API consente di accedere a una vasta gamma di informazioni sui canali, tuttavia, è importante notare che c'è un limite massimo di 10.000 query al giorno per ogni chiave API, il che impone una restrizione sul numero di richieste che possono essere effettuate (il limite può essere cambiato ma ai fini del progetto sono rimasto entro le 10000 query al giorno).\n",
    "\n",
    "Nel mio script, ho estratto i seguenti parametri chiave per ogni canale:\n",
    "\n",
    "- ChannelID: l'ID unico del canale, ottenuto dal campo channel['channelId'].\n",
    "- Title: il titolo del canale.\n",
    "- Country: il paese di origine del canale.\n",
    "- Subscribers: il numero di iscritti al canale.\n",
    "- TotalVideo: il numero totale di video caricati sul canale.\n",
    "- Views: il numero totale di visualizzazioni che i video del canale hanno accumulato.\n",
    "- Join_date: la data in cui il canale è stato creato.\n",
    "\n",
    "Da notare che i parametri <i> Subscribers </i> e <i> Views </i> sono stati utilizzati per la predizione della popolarità del canale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/stonks.jpg\"  /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logstash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Kafka (https://kafka.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/kafka.jpg\"  /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Kafka è una piattaforma distribuita basata su messaggi e sul modello publish-subscribe, facilita la scalabilità e l'affidabilità nell'invio e nella ricezione di messaggi tra applicazioni, fornendo una piattaforma robusta per l'integrazione e l'elaborazione dei dati in tempo reale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concetti chiave di Kafka:\n",
    "\n",
    "- **Producer**: Chi invia i dati al sistema Kafka. Il producer determina su quale topic inviare il messaggio.\n",
    "\n",
    "- **Topic**: Rappresenta l'argomento a cui vengono inviati i messaggi dai producer e da cui i consumer leggono i messaggi. È l'unità fondamentale di categorizzazione dei dati.\n",
    "\n",
    "- **Partizione**: È una struttura di dati all'interno di un topic in cui vengono scritti i messaggi. Ogni topic può essere suddiviso in una o più partizioni, consentendo una distribuzione dei dati scalabile e parallela.\n",
    "\n",
    "- **Consumer**: Chi legge i messaggi da uno o più topic. Ogni consumer appartiene a un gruppo di consumer, che cooperano per consumare i messaggi in modo distribuito e parallelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/struttura_kafka.png\" width=800 height=500 /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro caso:\n",
    "- **Producer**: Logstash\n",
    "- **Topic**: taptube_channel\n",
    "- **Consumer**: Apache Spark\n",
    "\n",
    "Inoltre kafka, all'interno del nostro progetto, è cosi suddiviso:\n",
    "\n",
    "- **Zookeper**: servizio per la sincronizzazione dei consumer;\n",
    "- **Broker**: Cuore di Kafka, gestisce il flusso di dati e mantiene storati i messaggi in attesa dei consumatori.\n",
    "- **Init-Kafka**: Script per l'inizializzazione di topic, partition..\n",
    "- **Kafka-UI**: interfaccia Web relativa a Kafka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Spark (https://spark.apache.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/spark.png\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark è un framework open-source progettato per il calcolo distribuito e il trattamento di dati su larga scala. È progettato per essere veloce, scalabile e offre un'interfaccia unificata per il trattamento di dati sia batch che in tempo reale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/struttura_spark.png\"  /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per allenare il modello si è scelto di utilizzare il dataset fornito da Kaggle (https://www.kaggle.com/datasets/asaniczka/2024-youtube-channels-1-million/data), però è stato necessario affrontare diversi passaggi per preparare i dati in modo appropriato per l'addestramento e l'elaborazione in tempo reale tramite Kafka; rimuovere NaN, gestire gli outliers, ridurre le dimensioni del dataset, rimuovere le features non importanti ne sono alcuni esempi per rendere il nostro dataset più ***light***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Databriks\n",
    "Databricks è una piattaforma di analisi dei dati basata su Apache Spark, progettata per semplificare lo sviluppo, la gestione e il monitoraggio di applicazioni di big data e machine learning.\n",
    "\n",
    "Ho utilizzato Databriks (la Community Edition) per testare il dataset con la quale allenare il modello prima di dockerizzare il tutto e non far implodiere il mio pc, ma c'è stato un piccolo problema con la grandezza del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/databriks.jpg\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Risorse limitate.... </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per l'allenamento del modello è stato utilizzato PySpark ed in particolare la libreria Spark MLlib utilizzando la funzione \"KMeans\" all'interno di una pipeline di machine learning. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Assemblare le feature in un vettore\n",
    "assembler = VectorAssembler(inputCols=['Subscribers', 'Views'], outputCol='features')\n",
    "\n",
    "# Algoritmo di Clustering \"kmeans\"\n",
    "kmeans = KMeans(featuresCol=\"features\", k=3)\n",
    "\n",
    "# setup the pipeline\n",
    "model = Pipeline(stages=[assembler, kmeans])\n",
    "\n",
    "#trained model\n",
    "pipelineFit = model.fit(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Quando avviene la chiamata pipeline.fit(), viene adattato il modello al training set di input e gli stage vengono eseguiti in ordine per ogni record del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/training.jpg\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Il modello prodotto ha ottenuto un'accuracy del 80%  circa ed in generale delle buone performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allenato il modello si è passati all'utilizzo di Spark Structured Streaming, che ci permette di lavorare su dati in tempo reale, integrandosi perfettamente con Kafka.\n",
    "\n",
    "Ogni dato ricevuto, viene passato al modello, il quale effettua la predizione.\n",
    "\n",
    "I dati predetti vengono poi depositati su elasticsearch e quindi visualizzati con kibana.\n",
    "\n",
    "\n",
    "trained model\n",
    "prediction = model.transform(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization: ElasticSearch + Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "AHHH finalmente, il tutto è stato configurato ora basta che passo tutto a kibana con elestic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/dockertoresource.jpg\" width=600 height=400 /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/elastickibana_2.png\" width=700 height=200 /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elasticsearch** è un motore per la ricerca e l'analisi di dati: è in grado di gestire tutte le tipologie di dato (testuale, numerico, geospaziale, strutturato e non strutturato) ed è conosciuto per la sua natura distribuita, velocità e scalabilità.\n",
    "\n",
    "Elasticsearch è stato utilizzato per indicizzare e memorizzare i dati precedentemente elaborati e inviarli a Kibana per visualizzarli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kibana** è una UI utilizzata per visualizzare ed analizzare i dati di Elasticsearch con l'ausilio di grafici.\n",
    "\n",
    "Di seguito vengono riportati alcuni grafici mostrati nella dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/ciaoo.jpg\" /> </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
